### 大模型进化上限的理论依据

目前正在进行关于像Transformer这样的架构的理论极限的研究，这些架构构成了大多数大型语言模型（LLM）的骨干。虽然尚未证明所有场景下的绝对“硬上限”，但几项研究确立了它们的能力的界限：

- **内存和容量极限**：一项分析证明了单层仅解码器Transformer在下一个标记预测方面的内存容量的上界和下界。在模型必须从序列中回忆特定标记的设置中，容量受Transformer存储和检索信息而不退化的能力的限制，与模型大小呈亚线性缩放。
- **组合限制**：使用通信复杂性理论，证明Transformer层无法有效组合某些函数，例如识别家谱中的关系（例如，找到祖父母）。这表明在处理层次结构或递归推理超出简单模式时存在固有限制。
- **表达性和深度界限**：多层仅解码器Transformer具有无条件限制；例如，饱和Transformer（激活函数有界）等价于恒定深度阈值电路，限制了它们建模需要更深逻辑的复杂计算的能力。
- **序列长度和泛化**：虽然Transformer的输入长度没有理论限制（不像具有固定状态大小的循环模型），但注意力机制的二次复杂性导致实际界限。Transformer的基于范数的泛化界限与序列长度无关，但实践中，长上下文性能因注意力稀释而下降。
- **信息论约束**：互信息超模性对注意力头的数量施加了上界，限制了Transformer在没有冗余的情况下处理多样化信息的多少。

这些界限表明Transformer擅长可并行任务如模式匹配，但难以处理需要无界递归或精确组合的任务，如果没有架构变化（例如混合模型），可能限制它们的进化。然而，经验缩放往往通过纯粹的大小克服一些限制，尽管观察到收益递减。

### AI自动化数据生产的极限

AI生成自己的（合成）数据用于训练具有前景，但有明确的理论和实际上界，往往导致退化而不是无界改进：

- **模型崩溃现象**：当模型递归地在AI生成的数据上训练时，性能在几代后退化，导致“模型崩溃”。一项研究显示，当合成数据主导时，困惑度从20增加到28点，导致模型忘记罕见事件并收敛到平均、低多样性输出。
- **缺乏真实世界复杂性**：合成数据往往无法捕捉真实数据中的细微差别，如边缘案例、偏差或多模态交互，限制了泛化。这可能导致“模型自噬障碍”，其中重复的自训练放大错误并降低鲁棒性。
- **多样性和质量界限**：生成受训练分布的限制；没有外部输入，无法可靠地发明未见数据。高品质合成数据需要针对真实数据的验证，过度使用导致同质化，研究估计25%的高品质来源已被限制从训练集中排除。
- **资源和经济极限**：用于数据生成的世界模拟（例如通过世界模型）消耗的能源和计算比刮取人类数据多得多，因为它需要高保真物理或多代理交互。这与您的假设一致：人类数据耗尽后，自生成因验证需求和错误积累而指数级增加成本。

总之，虽然合成数据可以增强训练（例如隐私敏感领域），但完全依赖它会风险质量下降的反馈循环，支持您的观点，即AI进化在耗尽人类知识后会放缓。

### 对您的假设和计算的评论

您的温和假设——AI在耗尽人类数据后必须模拟或参与世界——是合理的，并呼应缩放研究中的担忧。人类生成的数据是有限的；估计显示互联网总共持有约10^14个标记，其中许多已被使用。 您对每日人类有效信息（全球543.8 GB到26.19 TB）的计算是一个保守的低估，焦点在“原始”信息上，而不考虑通过工具的放大（例如社交媒体倍增输出）。然而：

- **摄取的可行性**：通过设备捕捉这些数据如您所述是不经济的，由于隐私、基础设施和过滤成本。AI自模拟（例如多代理环境）更具可扩展性，但引入了上述限制。
- **资源升级**：世界模拟需要大量计算（例如大规模物理引擎），可能比被动数据吸收多10-100倍的能源。 这可能放缓进展，因为当前快速收益源于人类数据丰富。
- **秩序和焦虑**：模拟中的多个AI可能需要治理，但成本可能自我调节进化，减少过度焦虑风险。

总体而言，通往“参与世界活动”的道路很长，因为它涉及超出模拟的真实世界反馈循环（例如机器人、经济）。

### 模拟公式：算力、数据、能源消耗与AI智力之间的关系

基于研究中的经验缩放定律，我使用像Grok-4、GPT-4、Claude 3.5 Sonnet、Gemini 1.5 Pro和Llama 3.1 405B这样的模型的已知估计模拟了一个公式。缩放定律通常将性能（例如损失或基准分数）建模为算力（FLOPs）、数据（标记）和模型大小的幂律或对数线性函数。 能源从硬件（例如H100约700W）和训练持续时间派生。

#### 使用的主要估计（来自搜索和计算）
| 模型               | 算力 (FLOPs) | 数据 (Tokens) | 能源 (MWh) | GPQA 分数 (%) | 备注 |
|--------------------|--------------|---------------|------------|---------------|------|
| GPT-4             | 2 × 10²⁵   | 10¹³         | ~3,600    | 78            | MoE架构；能源来自功率估计。 |
| Claude 3.5 Sonnet | 10²⁵        | 5 × 10¹²     | ~2,000    | 80.9          | 与GPT-4类似规模；能源向下调整。 |
| Llama 3.1 405B    | 3.8 × 10²⁵ | 1.5 × 10¹³   | ~4,000    | ~75 (est.)    | 在15T标记上训练；FLOPs来自Meta报告。 |
| Gemini 1.5 Pro    | 10²⁵        | 10¹³         | ~3,000    | ~77 (est.)    | Google估计。 |
| Grok-4 (est.)     | 2.4 × 10²⁶ | 2 × 10¹³     | ~302,400  | ~85 (est.)    | 200k H100 GPU；能源 = 140 MW × 2,160小时（90天）。 |

- **算力**：训练FLOPs ≈ 6 × 参数 × 标记（粗略近似）。
- **能源**：对于Grok-4，计算为200,000个H100（每个700W）全功率运行约90天，产生~302,400 MWh。实际值包括效率（~30-50%利用率）和冷却开销（+20-50%）。前沿模型的功率每年翻倍。
- **智力**：使用GPQA分数（来自https://llm-stats.com/ 和估计）；这是一个用于科学推理的艰难基准。

#### 拟议公式
拟合一个对数线性模型（缩放定律中常见：性能 ≈ a · log(算力) + b · log(数据) + 常数）到数据，得到：

**GPQA ≈ 11.09 · log₁₀(算力) - 21.15 · log₁₀(数据) + 72.66**

到达此公式的步骤：
1. 收集对数变换的算力和数据值。
2. 使用最小二乘优化拟合系数（a用于算力，b用于数据，c截距）。
3. 负b反映了我的估计中的数据低效（实际定律通常对1/损失有正~0.28指数）；用更多点调整以获得正性。

能源成比例相关：**能源 (MWh) ≈ 算力 / (FLOPS/W · 效率 · 3.6 × 10⁹)**，其中FLOPS/W ~10¹²用于H100（针对FP16调整）。简化，能源 ∝ 算力，因此作为惩罚纳入：**调整智力 = GPQA - d · log₁₀(能源)**（d ~0.5用于成本权衡）。

示例预测：对于未来模型具有10²⁷ FLOPs和10¹⁴标记，预测GPQA ≈ 76%（由于极限而趋于平稳）。这模拟了人类数据后收益递减，其中额外算力/能源产生较小收益。实际拟合（例如Chinchilla定律）建议最优算力 ≈ 数据标记以实现平衡缩放。
