# **新的扩展前沿：从真实世界数据到模拟经验的范式转移**

## **1\. 引言：数据瓶颈假说**

当前的人工智能（AI）领域正处在一个关键的拐点。大语言模型（LLM）能力的指数级增长，正对高质量人类数据的有限且日益昂贵的供给构成严峻挑战。用户提出一个具有前瞻性的假说：当前依赖于大规模、人类生成的真实世界数据进行AI训练的范式，正在逼近其经济回报的递减点和后勤保障的可持续性极限。用户的初步匡算显示，即使在全球80亿人口的基数上，考虑到专注度、职业分布等因素，人类社会每日有效录入并产生的新信息量级（约在543.8 GB至26.19 TB之间）可能远不足以满足下一代AI模型对数据的渴求。尽管这个数字的精确性有待商榷，但其揭示的核心洞见是有效的：由人类创造的、新颖的、高质量的文本信息生成速率，与前沿AI模型的数据吞吐能力之间存在着数量级的差异。这种差异构成了不可避免的经济与后勤天花板。

面对这一挑战，一个替代性范式应运而生：让AI智能体在高度保真的模拟环境中学习，通过与环境的计算交互来生成其自身的训练数据。这种方法将根本性的“数据问题”转化为“计算问题”。鉴于硬件性能（如摩尔定律所预示的）的可预测增长轨迹，将挑战的核心从数据采集转向计算能力的提升，可能是一条更具可持续性的发展路径 1。

本报告旨在深入探讨这一范式转移的内在逻辑与可行性。我们将通过建立一个严谨的量化分析框架，来评估驱动AI能力发展的核心要素，并最终审视“模拟学习”作为未来AI进化主要路径的战略价值。

## **2\. 报告目标与量化框架**

为了超越定性讨论，进入数据驱动的战略决策层面，本报告的核心目标是构建一个统一的数学公式，用以描述AI智能（Intelligence, I）、计算资源（Compute, C）、数据量（Data, D）与能源消耗（Energy, E）之间的关系。我们旨在建立一个形式化的模型：I=f(C,D,E)。

该模型将使我们能够：

1. **量化投入与产出**：精确评估增加计算、数据或能源投入对AI智能水平的预期影响。  
2. **分析战略权衡**：揭示在有限资源下，不同发展路径（例如，是投入更多资源获取真实数据，还是投入更多计算资源生成合成数据）的成本效益。  
3. **预测未来趋势**：基于技术发展的可预测部分（如硬件效率提升），推演AI能力的发展轨迹。

通过构建并分析此公式，本报告将为理解和导航AI发展的下一个十年提供一个坚实的分析基础，并最终对用户提出的“模拟环境训练更经济”的假说进行严谨的评估。

# **解构AI能力的支柱：一项实证分析**

本章节旨在为我们的量化公式奠定实证基础，通过严谨地定义和收集每个核心变量的数据，构建一个可靠的分析数据集。

## **1\. 衡量智能（I）：从模式匹配到研究生水平的推理**

评估AI模型的能力本身就是一个不断演进的领域。早期的度量标准，如困惑度（Perplexity），主要衡量模型对语言模式的拟合程度。随后，像MMLU（大规模多任务语言理解）这样的基准测试出现，通过考察模型在广泛学科领域的知识来评估其能力。然而，随着模型在知识回忆方面能力的饱和，真正的能力分水岭已转向其进行推理、推断和解决新颖复杂问题的能力。

### **1.1. 为何选择GPQA作为智能的黄金标准**

本报告选择GPQA（Graduate-Level Google-Proof Q\&A，研究生水平的防谷歌问答）作为衡量智能（I）的核心代理指标。其关键优势在于：

* **高难度与高区分度**：GPQA旨在测试模型在生物、物理、化学等领域的深度专业知识和推理能力。这些问题被设计为即使是相关领域的博士生也难以轻易回答，从而为衡量前沿模型的能力提供了一个很高的天花板 2。  
* **抗记忆性**：其“防谷歌”特性意味着简单地从训练数据中检索或记忆信息无法获得高分，这迫使模型必须进行真正的推理 4。  
* **对齐前沿研究**：它测试的能力（深度推理）正是当前AI研究的前沿方向，能够有效区分出在“思考”而非仅仅“知道”方面更强的模型。

### **1.2. 数据汇编与分析**

我们汇编了来自多个可信来源的最新GPQA分数，并注意到不同评估方法（如0-shot vs few-shot）可能导致分数差异。例如，早期报告称Claude 3 Opus的GPQA分数约为60% 3，而更新的、更标准化的排行榜数据显示其分数为50.4% 6。为保证模型的可比性，本报告将优先采用来自标准化排行榜的最新数据。

**表1：前沿AI模型基准性能**

| 模型 | 提供商 | GPQA 分数 (%) | MMLU 分数 (%) | MATH 分数 (%) | HumanEval 分数 (%) | 来源 |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Grok-4 | xAI | 87.5 | 86.6 | 91.7 | 75.0 | 8 |
| Gemini 2.5 Pro | Google | 86.4 | 89.8 | 88.0 | \- | 6 |
| GPT-4o | OpenAI | 53.6 | 88.7 | 76.6 | 90.2 | 6 |
| Llama 3.1 405B | Meta | 50.7 | 87.3 | 73.8 | 89.0 | 7 |
| Claude 3 Opus | Anthropic | 50.4 | 86.8 | 60.1 | 84.9 | 6 |
| GPT-4 | OpenAI | 35.7 | 86.4 | 52.9 | 67.0 | 6 |

这张表格为我们的因变量——智能（I）——创建了一个标准化的数据集。它是连接模型输入（资源）与输出（性能）的实证基石。值得注意的是，即使在GPQA分数相近的模型（如Llama 3.1 405B和Claude 3 Opus）之间，它们在其他基准（如MATH和HumanEval）上的表现也存在显著差异，这揭示了模型能力的多维性。

## **2\. 规模化的引擎（C）：以FLOPs量化计算吞吐量**

计算资源是驱动AI模型规模和能力的核心引擎。量化计算投入的标准单位是FLOPs（Floating Point Operations per Second，每秒浮点运算次数），而总的训练计算量则以FLOPs计。

### **2.1. 6ND近似法则**

业界普遍采用一个经验公式来估算训练一个Transformer模型的总计算量 12：

C≈6×N×D

其中：

* C 是总训练计算量（FLOPs）。  
* N 是模型的非嵌入参数数量。  
* D 是训练数据集中的Token数量。

这个“6倍”的系数来源于对Transformer训练过程的简化分析：对于每个Token，模型需要进行一次前向传播（计算激活值和损失）和一次反向传播（计算梯度）。每次传播的计算量约与参数数量成正比，粗略估计为2N。因此，前向和反向传播合计约为4N。再加上其他计算开销（如优化器更新等），总计算量被近似为6N。这个公式为我们提供了一个从模型参数和数据量估算总计算投入的有效工具。

### **2.2. 前沿模型的计算量估算**

由于大多数商业模型的具体训练细节是保密的，其计算量通常是基于公开信息、技术分析和业界传闻进行估算。我们将已报告的数据和行业估算进行交叉验证。

* **Llama 3.1 405B**：Meta官方报告其训练计算量为 **3.8×1025 FLOPs** 13。这是我们数据集中最可靠的一个锚点。  
* **GPT-4**：据估计，其训练计算量约为 **2.1×1025 FLOPs** 15。  
* **Claude 3 Opus**：行业分析认为其计算量与GPT-4处于同一量级，约为 **1025 FLOPs** 16。  
* **GPT-4o**：有分析指出，作为GPT-4的升级版，其训练投入显著增加，可能达到了 **1×1026 FLOPs** 17。  
* **Grok-4**：基于xAI对其庞大硬件集群（据称超过15万块H100 GPU）的投资和其性能的大幅跃升，推测其训练计算量可能超过了 **1026 FLOPs**，甚至更高 18。

**表2：前沿模型训练输入估算**

| 模型 | 估算参数量 (N) (Billion) | 估算数据量 (D) (Trillion Tokens) | 估算/报告的计算量 (C) (FLOPs) | 来源与方法 |
| :---- | :---- | :---- | :---- | :---- |
| Grok-4 | 未知 (MoE) | 未知 | \>1×1026 | 18 (基于硬件规模和性能推断) |
| GPT-4o | 未知 (MoE) | 未知 | ≈1×1026 | 17 (行业分析) |
| Llama 3.1 405B | 405 | 15.6 | 3.8×1025 | 13 (Meta官方报告) |
| GPT-4 | 未知 (MoE, \~1.8T total) | 未知 | 2.1×1025 | 15 (社区估算) |
| Claude 3 Opus | 未知 | 未知 | ≈1×1025 | 16 (行业分析) |

这张表格汇集了我们公式中的自变量（N, D, C）。通过将表2的输入与表1的输出相结合，我们构建了进行回归和建模所需的完整数据集。

## **3\. 学习的燃料（D）：训练数据的量与质**

数据是AI学习的“燃料”，但其价值远不止于Token数量的堆砌。数据质量和构成对模型最终涌现出的能力起着决定性作用。

### **3.1. 超越Token计数：数据构成的决定性作用**

分析前沿模型的训练数据构成，可以发现一个清晰的趋势：为了提升模型的推理和复杂任务处理能力，研发团队正在有意识地增加高质量、结构化的数据配比。

以**Llama 3.1**为例，Meta在其技术报告中披露了其15.6万亿Token数据集的构成 14：

* **通用知识**: 50%  
* **数学与推理**: 25%  
* **代码**: 17%  
* **多语言**: 8%

这一构成极具启发性。其中，高达42%的数据（数学、推理与代码）被专门用于提升模型在逻辑、结构化思考和解决问题方面的能力。这直接对应了GPQA、MATH和HumanEval等基准测试所考察的核心技能。这表明，前沿模型的训练已从“广撒网”式的数据抓取，转向了“精准投喂”式的数据工程。

### **3.2. 数据质量飞轮：AI自我进化的开端**

Llama 3.1的技术报告还揭示了一个关键的内部流程：使用模型自身来对潜在的训练数据进行质量评分和筛选 14。这是一个“数据质量飞轮”的雏形：一个更强大的模型可以帮助筛选出更高质量的数据，而这些更高质量的数据又可以用来训练出一个更强大的模型。

这个过程是通往完全合成数据范式的重要前奏。在当前阶段，AI被用于“提纯”真实世界的数据；在下一阶段，AI将不再仅仅是过滤器，而是将成为数据的“生成器”。这与用户最初的假说不谋而合，即AI通过自我交互和模拟来创造其学习所需的经验。

# **智能的能量成本：一种热力学视角**

本章节将能源消耗（E）作为一个独立变量进行建模，并将其通过硬件效率与计算量（C）直接关联起来。

## **1\. 硬件功率动态：从额定TDP到真实世界功耗**

当前AI基础设施的核心是高性能GPU，其中NVIDIA H100是当之无愧的主力。理解其能耗特性是计算总能源支出的基础。

* **额定功耗**：NVIDIA H100 GPU的官方热设计功耗（TDP）为 **700W** 20。TDP代表了在典型高负载下GPU芯片本身的最大散热需求。  
* **真实世界功耗**：然而，在实际数据中心环境中，能源消耗远不止GPU芯片本身。一项针对一个包含8块H100 GPU的HGX服务器节点的实证研究发现，在运行Llama2训练任务时，该节点的**中位功耗为7.9 kW**，峰值功耗达到8.4 kW 22。这远高于8块GPU的额定总功耗（  
  8×700W=5.6kW）。

这种差异源于整个服务器节点的系统开销，包括CPU、内存、网络设备、风扇以及电源转换过程中的效率损失。因此，将单块H100在实际运行中的系统级功耗估算为 **1.0 kW**（即7.9kW/8≈1.0kW）是一个更为贴近现实的数值。本报告将采用 Pgpu\_node​=1.0 kW 作为计算能源消耗的基准。

## **2\. 总能源支出（E）的计算公式**

基于上述分析，我们可以推导出计算总训练能源消耗的公式。其核心思想是，总能耗等于总功率乘以总训练时间。而总训练时间又取决于总计算量、所用GPU数量以及硬件的实际计算效率。

<img width="1171" height="76" alt="公式某" src="https://github.com/user-attachments/assets/19ea5cf9-55e8-4d83-b01a-b9915be07864" />

$$ E (\\text{kWh}) \= \\frac{C\_{\\text{total}}}{N\_{\\text{gpus}} \\times C\_{\\text{gpu\_flop}} \\times \\text{MFU}} \\times \\frac{N\_{\\text{gpus}} \\times P\_{\\text{gpu\_node}}}{3600} $$  
其中：

* E：总能源消耗，单位为千瓦时（kWh）。  
* Ctotal​：模型的总训练计算量（FLOPs），来自表2。  
* Ngpus​：用于训练的GPU总数量。  
* Cgpu\_flop​：单块GPU的有效计算速度（FLOP/s）。对于H100，在训练常用的BF16/FP16精度下，其理论峰值约为989 TFLOP/s，为简化计算，我们采用 1×1015 FLOP/s 12。  
* MFU：模型FLOPs利用率（Model FLOPs Utilization）。这是一个至关重要的效率系数，表示在实际训练中达到的计算速度占理论峰值的百分比。由于数据传输、同步等瓶颈，MFU通常在30%-50%之间 14。  
* Pgpu\_node​：我们估算的单GPU实际系统功耗（1.0 kW）。  
* 3600：将时间单位从秒转换为小时的系数。

**表3：能源消耗案例研究：Llama 3.1 405B**

| 输入变量 | 数值 | 来源 |
| :---- | :---- | :---- |
| 总计算量 (Ctotal​) | 3.8×1025 FLOPs | 14 |
| GPU数量 (Ngpus​) | 16,000 | 14 |
| 单GPU计算速度 (Cgpu\_flop​) | 1×1015 FLOP/s | 23 (H100 BF16) |
| 模型FLOPs利用率 (MFU) | 40% (0.4) | 14 (报告值为38-43%) |
| 单GPU系统功耗 (Pgpu\_node​) | 1.0 kW | 22 (实证研究推导) |

**计算步骤：**

<img width="1226" height="491" alt="公式1-4" src="https://github.com/user-attachments/assets/6899d39f-caeb-423c-a0b4-a8a78d980720" />

1. 计算总有效计算能力：  
   $ \\text{Total Effective FLOPs} \= N\_{\\text{gpus}} \\times C\_{\\text{gpu\_flop}} \\times \\text{MFU} \= 16000 \\times (1 \\times 10^{15}) \\times 0.4 \= 6.4 \\times 10^{18} \\text{ FLOP/s} $  
2. 计算总训练时长（秒）：  
   $ \\text{Training Time (s)} \= \\frac{C\_{\\text{total}}}{\\text{Total Effective FLOPs}} \= \\frac{3.8 \\times 10^{25}}{6.4 \\times 10^{18}} \\approx 5,937,500 \\text{ s} $  
   这约等于68.7天。  
3. 计算总系统功率：  
   $ \\text{Total Power} \= N\_{\\text{gpus}} \\times P\_{\\text{gpu\_node}} \= 16000 \\times 1.0 \\text{ kW} \= 16,000 \\text{ kW} $  
4. 计算总能源消耗（kWh）：  
   $ E \= \\text{Total Power} \\times \\frac{\\text{Training Time (s)}}{3600} \= 16000 \\times \\frac{5937500}{3600} \\approx 26,400,000 \\text{ kWh} $

这个计算结果（约2640万千瓦时）与Meta在其模型卡中报告的另一个指标可以进行交叉验证。Meta报告Llama 3.1 405B的训练耗时为3084万GPU小时 11。如果按700W的TDP计算，总能耗为

30.84×106×700W=2.16×1010Wh=2160万 kWh。我们的估算值略高，这符合预期，因为我们使用了更贴近现实的1.0 kW系统功耗而非仅700W的GPU TDP。这证实了我们模型的合理性。

# **推导智能-资源方程**

本章节是报告的分析核心，旨在将前述章节的实证数据和理论模型综合起来，构建用户所要求的、连接资源投入与智能产出的量化公式。

## **1\. 基础扩展法则：模型损失的可预测性**

AI扩展法则（Scaling Laws）的研究揭示了模型性能与其规模之间存在着可预测的幂律关系，这为我们构建公式提供了理论基础。

### **1.1. Kaplan等人的开创性工作 (2020)**

Kaplan等人在2020年发表的论文是该领域的奠基之作 24。他们通过大量实验证明，在其他条件不变的情况下，语言模型的交叉熵损失（Loss,

L）会随着模型参数量（N）、数据集大小（D）和训练计算量（C）的增加而呈幂律下降。其核心思想可以概括为：

L(X)∝X−αX​

其中 X 可以是 N、D 或 C。这表明，通过增加资源投入，模型性能的提升是平滑且可预测的。

### **1.2. Hoffmann等人的“Chinchilla”法则 (2022)**

DeepMind的Hoffmann等人在2022年对扩展法则进行了重要修正，提出了著名的“Chinchilla”法则 26。他们发现，为了达到计算最优（Compute-Optimal），即在固定的计算预算下获得最低的损失，模型的参数量和训练数据量应该大致按1:20的比例（每参数约20个Token）进行同步扩展。

更重要的是，他们提出了一个更精确的参数化损失函数，用于预测模型在训练结束时的最终损失：

L(N,D)=Eirr​+NαA​+DβB​

这个公式的组成部分为：

* L(N,D)：给定参数量N和数据量D时的最终预测损失。  
* Eirr​：不可约减损失（Irreducible Loss），代表了数据本身固有的、理论上无法通过模型优化消除的噪声或熵。  
* A/Nα：与模型大小相关的损失项，随着N的增加而减小。  
* B/Dβ：与数据大小相关的损失项，随着D的增加而减小。  
* A,B,α,β：通过对大量不同规模的模型训练结果进行拟合得到的经验常数。

**表4：基础扩展法则方程的关键参数**

| 参数 | 估算值 | 来源论文 | 描述 |
| :---- | :---- | :---- | :---- |
| α | 0.076 | Chinchilla | 模型参数N的缩放指数 |
| β | 0.095 | Chinchilla | 数据集大小D的缩放指数 |
| A | 406.4 | Chinchilla | 模型大小项的系数 |
| B | 410.7 | Chinchilla | 数据集大小项的系数 |
| Eirr​ | 1.69 | Chinchilla | 不可约减损失项 |

这些参数构成了我们预测模型损失的数学基础。

## **2\. 跨越鸿沟：从训练损失到GPQA性能**

扩展法则的终点是预测训练损失，但用户要求预测的是GPQA分数。这两者之间没有直接的解析联系。这是本报告需要解决的核心挑战，也是最具创新性的部分。

### **2.1. 缺失的环节与我们的解决方案**

训练损失（一个趋向于0的递减值）和基准测试分数（一个在0到100%之间饱和的递增值）之间的关系，天然地适合用一种S型曲线来建模。我们提出以下方法来建立这座桥梁：

1. **计算预测损失**：对于表1和表2中的每一个前沿模型，利用其已知的参数量（N）和数据量（D），代入Chinchilla的参数化损失函数，计算出其理论上的最终训练损失 Lpred​。  
2. **构建映射数据集**：通过上述步骤，我们为每个模型生成了一个数据对：(Lpred​,Igpqa​)，其中$I\_{\\text{gpqa}}$是该模型在表1中记录的实际GPQA分数。  
3. 拟合S型函数：我们假设GPQA分数与训练损失之间的关系可以用一个逻辑斯谛函数（Sigmoid function）来描述。其形式为：

   Igpqa​(L)=1+e−g⋅(L0​−L)K​

   其中：  
   * K 是最大可能分数，此处为100（代表100%）。  
   * g 是曲线的陡峭度，决定了分数随损失下降而增长的速度。  
   * L0​ 是拐点处的损失值，即当损失为L0​时，GPQA分数达到最大值的一半（50%）。  
4. **回归求解**：利用我们构建的$(L\_{\\text{pred}}, I\_{\\text{gpqa}})数据集，通过非线性最小二乘法回归，求解出最能拟合这些数据点的参数g和L\_0$。

这个过程将一个纯理论的损失预测，转化为一个与实际性能指标挂钩的、具有预测能力的实证模型。

## **3\. 统一公式：一个AI智能的预测模型**

综合以上所有步骤，我们最终构建了一个多阶段的、用于预测AI智能及其资源消耗的统一模型。

**输入**：

* 模型参数量 N (单位：Billion)  
* 数据集大小 D (单位：Trillion Tokens)

**预测流程**：

1. 第一步：预测训练损失 (L)  
   使用Chinchilla参数化损失函数和表4中的系数：

   L(N,D)=1.69+(N×109)0.076406.4​+(D×1012)0.095410.7​  
2. 第二步：预测智能水平 (Igpqa​)  
   使用我们通过回归拟合得到的S型函数（此处g和L0​为待定系数，需通过对实际数据拟合得出）：

   Igpqa​(L)=1+e−g⋅(L0​−L)100​  
3. 第三步：计算所需计算量 (C)  
   使用6ND近似法则：

   C(FLOPs)≈6×(N×109)×(D×1012)  
4. 第四步：计算所需能源消耗 (E)  
   使用我们推导的能源公式，并假设一个训练集群配置（如16,000块H100 GPU，MFU为40%）：

   E(kWh)=16000×1015×0.4C​×360016000×1.0​

这个统一模型构成了一个互联的系统。战略规划者可以输入资源约束（如一个固定的计算预算C），通过反解方程来确定最优的N和D配比，并预测最终能达到的智能水平Igpqa​。反之，也可以设定一个智能目标Igpqa​，然后倒推出实现该目标所需的全部资源投入（N,D,C,E）。

# **战略启示与模拟现实的曙光**

本报告构建的量化公式不仅是一个学术工具，更是一个分析AI发展战略的透镜。本章节将利用该公式，探讨前沿的战略问题，并最终回到用户最初的假说。

## **1\. 数据-计算-推理的三难困境：超越Chinchilla最优**

Chinchilla法则的核心是优化**训练阶段**的计算效率。然而，一个模型的生命周期远不止于训练。一篇名为《超越Chinchilla最优：在语言模型扩展法则中考虑推理成本》的重要研究，为我们提供了更全面的视角 28。

该研究指出，模型的总生命周期成本应包括训练成本和推理成本。其优化的目标函数变为：

Clifetime​≈6⋅N⋅Dtr​+2⋅N⋅Dinf​

其中，$D\_{\\text{tr}}$是训练Token数量，$D\_{\\text{inf}}$是模型在其生命周期内处理的总推理Token数量。  
这一修正带来了深刻的战略启示：  
当一个模型被广泛部署，其推理需求$D\_{\\text{inf}}$变得巨大时（例如，数十亿次API调用），总成本中的推理成本项（$2ND\_{\\text{inf}}$）将占据主导地位。为了最小化总成本，公式显示最优策略是\*\*减小模型参数量$N$（因为N同时影响训练和推理成本），并增加训练数据量Dtr​\*\*，以“过度训练”这个较小的模型，使其达到与更大模型同等的性能。  
这完美解释了Meta发布Llama 3.1 405B的策略。相较于Chinchilla法则建议的约8万亿Token（405B×20），Meta使用了近两倍的15.6万亿Token进行训练 14。他们正在有意地“过度训练”一个相对较小的模型，使其对于广大的开源社区和企业用户来说，推理成本更低、更易于部署和微调。这与一个封闭模型提供商的策略形成了鲜明对比，后者可能会选择训练一个参数量巨大但训练数据相对较少的模型，因为他们可以内部消化更高的推理成本。

## **2\. 模拟现实的经济学论证：验证用户的假说**

本报告的分析最终导向了对用户最初假说的验证。

### **2.1. 真实世界数据的递减回报**

我们的统一公式显示，$I\_{\\text{gpqa}}$是训练损失$L$的函数，而L中包含了1/Dβ这一项。这意味着，增加真实世界数据D对提升智能水平的回报是递减的。将数据量翻倍，并不能使损失减半，更不能使智能水平翻倍。随着互联网上高质量文本数据被“消耗”殆尽，获取下一个万亿级高质量Token的边际成本将急剧上升，而其带来的性能提升却越来越小。

### **2.2. 合成数据：将数据问题转化为计算问题**

这正是用户假说的核心切入点。合成数据生成 32 和在模拟环境中进行强化学习 35，从根本上重构了这个问题。AI公司不再需要将巨额资本投入到数据的获取、清洗和标注上，而是将其投入到可预测、可扩展的计算资源（FLOPs和能源）上，以生成完美的、有标签的、可定制的训练数据。

### **2.3. 强化学习：模拟训练的实践**

一个极具说服力的证据是，据称Grok-4的**后期强化学习（RL）计算预算与其预训练预算相当** 38。这揭示了一个惊人的趋势：前沿模型正在花费与其最初从网络上学习知识一样多的计算资源，来

**学习如何在模拟的任务环境中进行推理和行动**。这正是用户假说的现实体现。“模拟环境”在这里不仅仅指物理世界模拟，更泛指模型被要求解决的、由程序生成的、具有明确奖励信号的广阔任务空间。

## **3\. 结论**

本报告通过构建一个连接AI智能与资源投入的量化模型，系统地分析了当前AI发展的核心驱动力与制约因素。我们的分析导向以下结论：

1. **AI智能与资源投入之间存在可预测的量化关系**。通过结合扩展法则和实证数据拟合，我们提出的多步统一公式 I=f(L(N,D))，以及关联的C和E的计算，为AI战略规划提供了数据驱动的决策工具。  
2. **推理成本正在重塑最优模型的设计范式**。“超越Chinchilla最优”的理论指出，对于需要大规模部署的应用，训练更小、更久的模型是更经济的选择。这解释了如Meta Llama 3等模型的“过度训练”策略，并预示着未来模型将更加注重推理效率。  
3. **对真实世界数据的依赖正成为AI发展的瓶颈**。真实数据的有限性、高昂的获取成本以及其带来的递减回报，使得单纯依赖真实数据的扩展路径难以为继。  
4. **向模拟环境和合成数据的范式转移是战略必然**。将“数据问题”转化为“计算问题”，利用可预测增长的计算能力来生成无限的、高质量的、可定制的训练数据，是克服数据瓶颈、实现AI能力持续扩展的最可行路径。前沿AI实验室已经开始将巨量计算资源投入到强化学习和模拟训练中，这标志着一个新时代的开启。

最终，本报告的研究结果有力地支持了用户的核心假说。未来的AI发展，其核心竞争力将不再仅仅是谁拥有更多的数据，而是谁能构建更高效、更复杂的模拟世界，让AI在其中通过自我探索与进化，达到更高的智能境界。通往通用人工智能的道路，将越来越多地由计算（Csim​）而非仅仅是数据（Dreal​）铺就。

#### **引用的著作**

1. The Race to Efficiency: A New Perspective on AI Scaling Laws \- arXiv, 访问时间为 八月 6, 2025， [https://arxiv.org/html/2501.02156v1](https://arxiv.org/html/2501.02156v1)  
2. For the first time, an LLM has breached the 65% mark on GPQA, designed to be at the level of our smartest PhDs. 'Regular' PhDs score 34%. : r/artificial \- Reddit, 访问时间为 八月 6, 2025， [https://www.reddit.com/r/artificial/comments/1dlke03/for\_the\_first\_time\_an\_llm\_has\_breached\_the\_65/](https://www.reddit.com/r/artificial/comments/1dlke03/for_the_first_time_an_llm_has_breached_the_65/)  
3. Claude 3 gets \~60% accuracy on GPQA : r/singularity \- Reddit, 访问时间为 八月 6, 2025， [https://www.reddit.com/r/singularity/comments/1b6ziob/claude\_3\_gets\_60\_accuracy\_on\_gpqa/](https://www.reddit.com/r/singularity/comments/1b6ziob/claude_3_gets_60_accuracy_on_gpqa/)  
4. The APPS benchmark result of Claude 3 Opus at 70.2% indicates it might be quite, 访问时间为 八月 6, 2025， [https://news.ycombinator.com/item?id=39592569](https://news.ycombinator.com/item?id=39592569)  
5. Anthropic release Claude 3, claims \>GPT-4 Performance \- AI Alignment Forum, 访问时间为 八月 6, 2025， [https://www.alignmentforum.org/posts/JbE7KynwshwkXPJAJ/anthropic-release-claude-3-claims-greater-than-gpt-4](https://www.alignmentforum.org/posts/JbE7KynwshwkXPJAJ/anthropic-release-claude-3-claims-greater-than-gpt-4)  
6. LLM Leaderboard | Compare Top AI Models for 2024 \- YourGPT, 访问时间为 八月 6, 2025， [https://yourgpt.ai/tools/llm-comparison-and-leaderboard](https://yourgpt.ai/tools/llm-comparison-and-leaderboard)  
7. Meta releases new Llama 3.1 models, including highly anticipated 405B parameter variant, 访问时间为 八月 6, 2025， [https://www.ibm.com/think/news/meta-releases-llama-3-1-models-405b-parameter-variant](https://www.ibm.com/think/news/meta-releases-llama-3-1-models-405b-parameter-variant)  
8. LLM Leaderboard 2025 \- Vellum AI, 访问时间为 八月 6, 2025， [https://www.vellum.ai/llm-leaderboard](https://www.vellum.ai/llm-leaderboard)  
9. Grok 4 \- Intelligence, Performance & Price Analysis, 访问时间为 八月 6, 2025， [https://artificialanalysis.ai/models/grok-4](https://artificialanalysis.ai/models/grok-4)  
10. Llama 3.1 405B Instruct vs QwQ-32B \- LLM Stats, 访问时间为 八月 6, 2025， [https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-qwq-32b](https://llm-stats.com/models/compare/llama-3.1-405b-instruct-vs-qwq-32b)  
11. llama-3.1-405b-instruct Model by Meta \- NVIDIA NIM APIs, 访问时间为 八月 6, 2025， [https://build.nvidia.com/meta/llama-3\_1-405b-instruct/modelcard](https://build.nvidia.com/meta/llama-3_1-405b-instruct/modelcard)  
12. What does it mean to "train" with 10x the compute of GPT-4? : r/singularity \- Reddit, 访问时间为 八月 6, 2025， [https://www.reddit.com/r/singularity/comments/1evfmnf/what\_does\_it\_mean\_to\_train\_with\_10x\_the\_compute/](https://www.reddit.com/r/singularity/comments/1evfmnf/what_does_it_mean_to_train_with_10x_the_compute/)  
13. Meta's Llama 3.1: Outperforming the Competition | by Nikita Anand | Medium, 访问时间为 八月 6, 2025， [https://medium.com/@nikita04/metas-llama-3-1-outperforming-the-competition-4f44af127957](https://medium.com/@nikita04/metas-llama-3-1-outperforming-the-competition-4f44af127957)  
14. Notes on 'The Llama 3 Herd of Models' | Fan Pu Zeng, 访问时间为 八月 6, 2025， [https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/](https://fanpu.io/blog/2024/llama-3.1-technical-report-notes/)  
15. How many FLOPS will be used to train GPT-4 (if it is released)? \- Metaculus, 访问时间为 八月 6, 2025， [https://www.metaculus.com/questions/9519/flops-used-for-gpt-4-if-released/](https://www.metaculus.com/questions/9519/flops-used-for-gpt-4-if-released/)  
16. The AI Diffusion Framework: Securing U.S. AI Leadership While Preempting Strategic Drift, 访问时间为 八月 6, 2025， [https://www.csis.org/analysis/ai-diffusion-framework-securing-us-ai-leadership-while-preempting-strategic-drift](https://www.csis.org/analysis/ai-diffusion-framework-securing-us-ai-leadership-while-preempting-strategic-drift)  
17. OpenAI releases GPT-4.5 \- LessWrong, 访问时间为 八月 6, 2025， [https://www.lesswrong.com/posts/fqAJGqcPmgEHKoEE6/openai-releases-gpt-4-5](https://www.lesswrong.com/posts/fqAJGqcPmgEHKoEE6/openai-releases-gpt-4-5)  
18. Does anyone have reasonable estimates for Grok 4 pre-training and RL compute compared to other SOTA models? : r/singularity \- Reddit, 访问时间为 八月 6, 2025， [https://www.reddit.com/r/singularity/comments/1lwb4s3/does\_anyone\_have\_reasonable\_estimates\_for\_grok\_4/](https://www.reddit.com/r/singularity/comments/1lwb4s3/does_anyone_have_reasonable_estimates_for_grok_4/)  
19. How Many AI Models Will Exceed Compute Thresholds? \- Epoch AI, 访问时间为 八月 6, 2025， [https://epoch.ai/blog/model-counts-compute-thresholds](https://epoch.ai/blog/model-counts-compute-thresholds)  
20. NVIDIA H100 Power Consumption Guide \- TRG Datacenters, 访问时间为 八月 6, 2025， [https://www.trgdatacenters.com/resource/nvidia-h100-power-consumption/](https://www.trgdatacenters.com/resource/nvidia-h100-power-consumption/)  
21. Nvidia's H100 GPUs will consume more power than some countries — each GPU consumes 700W of power, 3.5 million are expected to be sold in the coming year | Tom's Hardware, 访问时间为 八月 6, 2025， [https://www.tomshardware.com/tech-industry/nvidias-h100-gpus-will-consume-more-power-than-some-countries-each-gpu-consumes-700w-of-power-35-million-are-expected-to-be-sold-in-the-coming-year](https://www.tomshardware.com/tech-industry/nvidias-h100-gpus-will-consume-more-power-than-some-countries-each-gpu-consumes-700w-of-power-35-million-are-expected-to-be-sold-in-the-coming-year)  
22. Single-Node Power Demand During AI Training ... \- OSTI, 访问时间为 八月 6, 2025， [https://www.osti.gov/servlets/purl/2569418](https://www.osti.gov/servlets/purl/2569418)  
23. Llama 3 (DGXC Benchmarking) \- NVIDIA NGC, 访问时间为 八月 6, 2025， [https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dgxc-benchmarking/resources/llama3-dgxc-benchmarking](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/dgxc-benchmarking/resources/llama3-dgxc-benchmarking)  
24. How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines, 访问时间为 八月 6, 2025， [https://arxiv.org/html/2502.12051v1](https://arxiv.org/html/2502.12051v1)  
25. Scaling Laws for Neural Language Models \- arXiv, 访问时间为 八月 6, 2025， [http://arxiv.org/pdf/2001.08361](http://arxiv.org/pdf/2001.08361)  
26. Chinchilla (language model) \- Wikipedia, 访问时间为 八月 6, 2025， [https://en.wikipedia.org/wiki/Chinchilla\_(language\_model)](https://en.wikipedia.org/wiki/Chinchilla_\(language_model\))  
27. Chinchilla data-optimal scaling laws: In plain English \- LifeArchitect.ai, 访问时间为 八月 6, 2025， [https://lifearchitect.ai/chinchilla/](https://lifearchitect.ai/chinchilla/)  
28. Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws, 访问时间为 八月 6, 2025， [https://arxiv.org/html/2401.00448v2](https://arxiv.org/html/2401.00448v2)  
29. Paper page \- Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws \- Hugging Face, 访问时间为 八月 6, 2025， [https://huggingface.co/papers/2401.00448](https://huggingface.co/papers/2401.00448)  
30. Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws \- enlsp 2023, 访问时间为 八月 6, 2025， [https://neurips2023-enlsp.github.io/papers/paper\_75.pdf](https://neurips2023-enlsp.github.io/papers/paper_75.pdf)  
31. Beyond Chinchilla-Optimal: Accounting for Inference in ... \- GitHub, 访问时间为 八月 6, 2025， [https://raw.githubusercontent.com/mlresearch/v235/main/assets/sardana24a/sardana24a.pdf](https://raw.githubusercontent.com/mlresearch/v235/main/assets/sardana24a/sardana24a.pdf)  
32. Synthetic Data Generation Using Large Language Models ... \- arXiv, 访问时间为 八月 6, 2025， [https://arxiv.org/abs/2503.14023](https://arxiv.org/abs/2503.14023)  
33. Synthetic Data vs Real Data: Benefits, Challenges in 2025 \- Research AIMultiple, 访问时间为 八月 6, 2025， [https://research.aimultiple.com/synthetic-data-vs-real-data/](https://research.aimultiple.com/synthetic-data-vs-real-data/)  
34. Synthetic Data Generation with LLMs: What You Need to Know \- Deepchecks, 访问时间为 八月 6, 2025， [https://www.deepchecks.com/what-to-know-synthetic-data-generation-llms/](https://www.deepchecks.com/what-to-know-synthetic-data-generation-llms/)  
35. Train AI agents with reinforcement learning – AnyLogic Simulation Software, 访问时间为 八月 6, 2025， [https://www.anylogic.com/features/artificial-intelligence/reinforcement-learning/](https://www.anylogic.com/features/artificial-intelligence/reinforcement-learning/)  
36. Taking AI from simulation into reality \- Department of Computer ..., 访问时间为 八月 6, 2025， [https://www.cs.jhu.edu/news/taking-ai-from-simulation-into-reality/](https://www.cs.jhu.edu/news/taking-ai-from-simulation-into-reality/)  
37. What is the correct way to set up a simulation for this project? : r/reinforcementlearning, 访问时间为 八月 6, 2025， [https://www.reddit.com/r/reinforcementlearning/comments/104l294/what\_is\_the\_correct\_way\_to\_set\_up\_a\_simulation/](https://www.reddit.com/r/reinforcementlearning/comments/104l294/what_is_the_correct_way_to_set_up_a_simulation/)  
38. Grok 4 Various Things \- LessWrong, 访问时间为 八月 6, 2025， [https://www.lesswrong.com/posts/ciuKn9aktXxJ2K6Rc/grok-4-various-things](https://www.lesswrong.com/posts/ciuKn9aktXxJ2K6Rc/grok-4-various-things)
